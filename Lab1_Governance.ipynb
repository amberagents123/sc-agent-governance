{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fa4882-3406-4a25-bf1c-dd5ff7728372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab 1: Building the Governance Foundation\n",
    "\n",
    "In this hands-on lab, you'll learn how to build AI agents that respect data governance policies using Databricks Unity Catalog. We'll create an HR Analyst agent that can answer questions about employees, compensation, and policies while automatically enforcing security controls through governance and identity best practices.\n",
    "\n",
    "### Lab Structure\n",
    "**Lab 1: Building the Governance Foundation**\n",
    "- Set up\n",
    "    - Create the `hr_data_analyst` service principal\n",
    "    - Create the `Devs` Group and add `hr_data_analyst` as a member\n",
    "    - Make sure you are connected to `serverless` compute\n",
    "    - Create HR tables in Unity Catalog as`clientcare.hr_data`\n",
    "    - Apply data classifications with tags\n",
    "    - Create a Analyst view with anonymization and filtering\n",
    "    - Give Catalog permissions to the `Devs` Group so they can access the view\n",
    "    - Implement table-level column masking for SSN\n",
    "    - Build functions for querying tables\n",
    "\n",
    "**Lab 2: Agent Evaluation and Testing**\n",
    "\n",
    "-   Creating the AI Agent with `Agent.py`: \n",
    "    - Compile the HR Analyst agent \n",
    "    - Bind the LLM, system prompt, and tools \n",
    "- Register the agent model in MLflow\n",
    "- Evaluate agent responses using LLM as a judge\n",
    "- Grant service principal the ability to run our code\n",
    "- Deploy the agent as a job on behalf of the service principal\n",
    "- Test the agent in Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0451cb73-aa09-4e0e-baac-577474ad883c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Initial Setup and Data Verification**\n",
    "\n",
    "Let's start by setting up our environment, this means getting data into our catalog and schema.\n",
    "\n",
    "_Background for those new to Databricks_\n",
    "\n",
    "- We use `spark.sql()` for all governance OPERATIONS (CREATE VIEW, GRANT, etc.)\n",
    "\n",
    "**Implementing Unity Catalog Governance: Available Methods**\n",
    "\n",
    "1. **Pure SQL** - The standard approach for production environments \n",
    "2. **Spark SQL in Python** - SQL commands as strings in Python (what we'll use)\n",
    "3. **PySpark DataFrame API** - The Pythonic approach for queries\n",
    "4. **Databricks SDK** - For reading data, automation and verification\n",
    "5. **REST API** - For external integrations and CI/CD pipelines\n",
    "6. **Terraform** - Infrastructure as Code approach for version-controlled, repeatable deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535067a0-b285-4e56-916c-fb07123f2849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set our working environment\n",
    "# Catalog and schema names\n",
    "catalog_name = \"clientcare\" #made up name for our company\n",
    "schema_name = \"hr_data\" #this our HR team's database\n",
    "\n",
    "# Create the catalog if it does not exist\n",
    "spark.sql(\n",
    "    f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\"\n",
    ")\n",
    "\n",
    "# Create the schema in the catalog\n",
    "spark.sql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f66ecd-1d08-436e-8c96-4598d11d5504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Local data folder\n",
    "data_dir = \"data/\"\n",
    "\n",
    "csv_files = {\n",
    "    \"compensation_data\": \"compensation_data.csv\",\n",
    "    \"employee_records\": \"employee_records.csv\", \n",
    "    \"hr_cases\": \"hr_cases.csv\",\n",
    "    \"internal_procedures\": \"internal_procedures.csv\",\n",
    "    \"performance_reviews\": \"performance_reviews.csv\",\n",
    "    \"public_policies\": \"public_policies.csv\"\n",
    "}\n",
    "\n",
    "# Pure Spark: Requires absolute paths, more complex file handling\n",
    "# This hybrid: pandas handles file I/O easily, Spark handles Unity Catalog integration\n",
    "\n",
    "# Load each CSV file\n",
    "for table_name, file_name in csv_files.items():\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    \n",
    "    # Read CSV and convert to Spark DataFrame in one line\n",
    "    spark_df = spark.createDataFrame(pd.read_csv(file_path))\n",
    "    spark_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.hr_data.{table_name}\")\n",
    "\n",
    "print(\"Tables created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19473b9f-86f5-4323-ba67-a8f104cc1fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "print(f\"{catalog_name} catalog and {schema_name} schema have been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a13a04-bab0-468e-b09b-901e80e51743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This demonstrates the current security risk - without proper governance controls,\n",
    "# all sensitive data (SSNs, salaries, bonuses) is fully exposed to any user with table access\n",
    "# In a real production environment, this would be a major compliance violation\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "   SELECT \n",
    "       e.employee_id,\n",
    "       e.first_name,\n",
    "       e.last_name,\n",
    "       e.department,\n",
    "       e.ssn,\n",
    "       e.phone,\n",
    "       e.email,\n",
    "       e.hire_date,\n",
    "       c.base_salary,\n",
    "       c.bonus,\n",
    "       c.stock_options\n",
    "   FROM employee_records e\n",
    "   JOIN compensation_data c ON e.employee_id = c.employee_id\n",
    "   ORDER BY e.employee_id\n",
    "   LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c9ee12-d6b6-4bad-a12a-0ee43394f092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Apply Data Classifications and Tags**\n",
    "\n",
    "First, we'll tag our tables with sensitivity levels. Table properties are metadata tags that help classify data sensitivity without changing the actual data structure:\n",
    "\n",
    "- Metadata key-value pairs attached to tables\n",
    "- Used by governance tools to understand data sensitivity\n",
    "- Don't affect table structure or data\n",
    "- Can be queried programmatically for compliance\n",
    "\n",
    "Our data classification schema:\n",
    " - Public: Anyone can access (company policies)\n",
    " - Internal: Employees only (procedures)\n",
    " - Confidential: Limited access (employee records, reviews)\n",
    " - Restricted: Highly sensitive (compensation, HR cases)\n",
    "\n",
    "Adding classification tags like \"Confidential,\" \"Restricted,\" or \"Public\" to Unity Catalog serve as metadata that make your data assets more manageable, secure, and compliant with both internal policies and external regulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b7cd4e-597b-418e-b586-444e140f5d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the classification tags for each table\n",
    "tables_config = [\n",
    "    (\"clientcare.hr_data.employee_records\", \"Confidential\", \"true\"),\n",
    "    (\"clientcare.hr_data.compensation_data\", \"Restricted\", \"true\"),\n",
    "    (\"clientcare.hr_data.performance_reviews\", \"Confidential\", \"true\"),\n",
    "    (\"clientcare.hr_data.hr_cases\", \"Restricted\", \"true\"),\n",
    "    (\"clientcare.hr_data.public_policies\", \"Public\", \"false\"),\n",
    "    (\"clientcare.hr_data.internal_procedures\", \"Internal\", \"false\")\n",
    "]\n",
    "\n",
    "# Apply classifications using tags (tags are metadata)\n",
    "for table_name, classification, has_pii in tables_config:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {table_name} SET TAGS (\n",
    "            'classification' = '{classification}',\n",
    "            'contains_pii' = '{has_pii}'\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(f\"‚úì Classified {table_name} as {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf093b4-5618-4fd5-8194-b563077c67e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Agent Permissions & Agent Access Requirements**\n",
    "Now that we've classified our data, let's design the right access level for our HR analytics agent through purpose-built views.\n",
    "\n",
    "**Key Decision:** The agent needs a specialized view - \"Data Analyst View\" - that provides analytical capabilities while protecting individual privacy.\n",
    "\n",
    "What the Agent Needs vs. Doesn't Need:\n",
    "| Data Type | HR Admin Sees | Manager Sees | Data Analyst Sees| Why? |\n",
    "|-----------|---------------|--------------|------------------|------|\n",
    "| **Names** | ‚úÖ John Smith | ‚úÖ John Smith | ‚ùå Anonymous IDs | Prevents bias, protects privacy |\n",
    "| **SSN** | ‚úÖ 123-45-6789 | ‚ùå Hidden | ‚ùå Hidden | No analytical value |\n",
    "| **Salary** | ‚úÖ $120,000 | ‚ùå Hidden | ‚úÖ $120,000 | Needed for accurate statistics |\n",
    "| **Department** | ‚úÖ Engineering | ‚úÖ Engineering | ‚úÖ Engineering | Needed for grouping |\n",
    "| **Performance** | ‚úÖ 4.5 | ‚úÖ 4.5 | ‚úÖ 4.5 | Needed for correlation analysis |\n",
    "| **Email** | ‚úÖ john@company.com | ‚úÖ john@company.com | ‚ùå Hidden | No analytical value |\n",
    "| **Phone** | ‚úÖ 555-1234 | ‚úÖ 555-1234 | ‚ùå Hidden | No analytical value |\n",
    "\n",
    "The Agent's Mission:\n",
    "Answer questions like:\n",
    "- \"What's the salary distribution in Engineering?\"\n",
    "- \"Is there pay equity across departments?\"\n",
    "- \"What's the correlation between performance and compensation?\"\n",
    "\n",
    "WITHOUT being able to answer:\n",
    "- \"What's John Smith's salary?\"\n",
    "- \"Who are the top 5 highest paid employees?\"\n",
    "- \"Show me SSNs for employees making over $100k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14dd10fc-f169-4790-afed-37b8c7e0a227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Create Classification-Aware View**\n",
    "\n",
    "Now we'll create the `data_analyst_view` that automatically provide appropriate data access based on our classifications. This view designed for analysts will have Anonymous IDs + full compensation data for statistical analysis.\n",
    "\n",
    "**Why Views for Access Control:**\n",
    "- **Simplicity**: Instead of complex masking rules for every possible user type, create purpose-built views\n",
    "- **Clarity**: Each view has a clear business purpose and user type\n",
    "- **Maintainability**: Easier to understand and modify than intricate permission matrices\n",
    "- **Performance**: Views are optimized SQL - no runtime masking overhead\n",
    "- **Security Layer**: In Databricks, views act as your security layer since we grant permissions directly to principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f42c0de-167d-45bc-a5a6-8596c9dbf798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DATA ANALYST VIEW - For statistical analysis \n",
    "\n",
    "# Key features: Anonymous employee IDs, full salary access, excludes Legal dept\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW data_analyst_view AS\n",
    "SELECT \n",
    "    CONCAT('EMP_', LPAD(e.employee_id, 6, '0')) as anonymous_id,  -- EMP_000001 format\n",
    "    e.department,\n",
    "    YEAR(e.hire_date) as hire_year,                            -- Year only for better anonymization\n",
    "    c.base_salary,                                              -- Full salary for analytics\n",
    "    c.bonus,\n",
    "    c.stock_options,\n",
    "    YEAR(c.effective_date) as comp_year,\n",
    "    pr.rating,\n",
    "    QUARTER(pr.review_date) as review_quarter,\n",
    "    YEAR(pr.review_date) as review_year\n",
    "\n",
    "FROM employee_records e\n",
    "LEFT JOIN compensation_data c ON e.employee_id = c.employee_id\n",
    "LEFT JOIN performance_reviews pr ON e.employee_id = pr.employee_id\n",
    "WHERE e.department != 'Legal'  -- Exclude Legal for compliance reasons\n",
    "\"\"\")\n",
    "print(\"‚úì Created data_analyst_view: Anonymous IDs + full compensation data (enhanced anonymization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c91bf6f-2f21-4ec4-94f7-fb1ca8041975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confirm view in the schema\n",
    "print(\"üìã Verifying our governance view was created successfully...\")\n",
    "view = spark.sql(f\"SHOW VIEWS IN {catalog_name}.{schema_name}\")\n",
    "display(view)\n",
    "\n",
    "print(\"\\n‚úÖ You should see:\")\n",
    "print(\"   - data_analyst_view (for agents and analysts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc75676a-bda2-439b-971b-2bff0e5cb728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test view\n",
    "# View the data\n",
    "df = spark.table(\"data_analyst_view\")\n",
    "df.display()\n",
    "\n",
    "# Check row count\n",
    "print(f\"View contains {df.count()} rows\")\n",
    "\n",
    "# Verify departments\n",
    "df.select(\"department\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de52aa7-c25d-4b5d-8ffa-f34aa69ae025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Configure Group Permissions**\n",
    "\n",
    "Now we'll grant permissions to the `Dev` group you have created (which is the group that you've added the service principal `HR_data_analyst` to -- in order to inherit the groups permission). The `Dev` group will get access to the view we created, plus the ability to create models using MLflow and register them in UC.\n",
    "\n",
    "This implements the principle of least privilege - the group only gets access to anonymized, aggregated data, never raw employee records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503407d1-a4ed-4811-a7a7-df8863905b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the Devs group exists\n",
    "groups_df = spark.sql(\"SHOW GROUPS\")\n",
    "display(groups_df)\n",
    "print(\"‚úÖ Confirmed: Devs group is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d732613c-acff-404d-9d1f-fc2734c4d533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you are granting wide permissions you can use: `spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name}.{schema_name} TO `{group_name}`\")` , and you will grant catalog permissions (which are inherited by all schemas) to the whole group. However, in this lab, we will grand minimum, specific access to the schema. \n",
    "\n",
    "**Container permissions** (Prerequisites)\n",
    "- USE CATALOG - Must be granted explicitly at catalog level\n",
    "- USE SCHEMA - Prerequisite to access any objects in the schema (tables, views, functions, models)\n",
    "\n",
    "**Object permissions** (Inheritable)\n",
    "- CREATE MODEL - Allows registering new MLflow models in the schema\n",
    "- CREATE MODEL VERSION - Allows adding new versions to existing registered models\n",
    "- SELECT - Allows reading data from tables/views (in your case, the data_analyst_view)\n",
    "- EXECUTE - Allows running functions and using deployed models for inference\n",
    "- CREATE TABLE - Allows creating new tables in the schema\n",
    "\n",
    "These permissions work together to let the Devs group build, register, deploy, and use ML models while accessing the necessary data through your view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf168de0-fb38-451f-8716-42bc5db5c193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "group_name = \"Devs\"\n",
    "view_name = \"data_analyst_view\"\n",
    "\n",
    "#devs need to see the data through the view, and need to build and register models into UC with MLflow\n",
    "\n",
    "spark.sql(f\"GRANT USE CATALOG ON CATALOG {catalog_name} TO `{group_name}`\")\n",
    "spark.sql(f\"GRANT USE SCHEMA ON SCHEMA {catalog_name}.{schema_name} TO {group_name}\")\n",
    "\n",
    "spark.sql(f\"GRANT CREATE TABLE ON SCHEMA {catalog_name}.{schema_name} TO `{group_name}`\")\n",
    "spark.sql(f\"GRANT EXECUTE ON SCHEMA {catalog_name}.{schema_name} TO `{group_name}`\")\n",
    "\n",
    "spark.sql(f\"GRANT CREATE MODEL ON SCHEMA {catalog_name}.{schema_name} TO `{group_name}`\")\n",
    "spark.sql(f\"GRANT CREATE MODEL VERSION ON SCHEMA {catalog_name}.{schema_name} TO `{group_name}`\")\n",
    "\n",
    "spark.sql(f\"GRANT SELECT ON VIEW {catalog_name}.{schema_name}.{view_name} TO `{group_name}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5fd098-fa19-453a-b47b-f494085059af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check schema-level permissions\n",
    "print(\"Schema-level permissions:\")\n",
    "spark.sql(f\"SHOW GRANTS ON SCHEMA {catalog_name}.{schema_name}\").show()\n",
    "\n",
    "# Check view-level permissions  \n",
    "print(f\"Permissions on {view_name} view:\")\n",
    "spark.sql(f\"SHOW GRANTS ON TABLE {catalog_name}.{schema_name}.{view_name}\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1611ca7d-cb31-4601-a04d-8cfe74e2a4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Column Masking**\n",
    "**Why implement this when we already have views?**\n",
    "\n",
    "Views can be bypassed if users gain direct table access. Column masking at the table level cannot be bypassed - it's enforced by Unity Catalog on every query, regardless of how the data is accessed.\n",
    "\n",
    "**How Table-Level Security Works:**\n",
    "- **Column Masking**: Functions run automatically transforming sensitive data based on who's viewing it\n",
    "- **Row-Level Security**: Functions can also control which rows users see (we already handled Legal department filtering in our views, but this could be done at the table level too)\n",
    "- Unity Catalog enforces these at the storage layer - users **cannot bypass** them\n",
    "- Different users see different versions of the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d56dfc6-4f1f-404f-8471-0d04678a9cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Implement column masking for SSN field - Defense in depth security\n",
    "print(\"üîê Implementing column masking for SSN field...\")\n",
    "\n",
    "# Create column masking function for SSN\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {catalog_name}.{schema_name}.mask_ssn(ssn_value STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE \n",
    "    WHEN is_account_group_member('Devs') THEN 'ANALYTICS_MASKED'\n",
    "    ELSE CONCAT('***-**-', RIGHT(ssn_value, 4))\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "# Apply the masking function to the SSN column\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {catalog_name}.{schema_name}.employee_records \n",
    "ALTER COLUMN ssn \n",
    "SET MASK {catalog_name}.{schema_name}.mask_ssn\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Column masking applied to SSN field\")\n",
    "print(\"üîê Defense in Depth: SSN now masked at table level\")\n",
    "print(\"   - Admins see full SSN\")\n",
    "print(\"   - devs group sees 'ANALYTICS_MASKED'\") \n",
    "print(\"   - Other users see '***-**-1234' format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93e6a1c-87f8-4a83-b8a5-3d20fee81beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate column masking implementation\n",
    "print(\"üß™ Testing SSN column masking...\")\n",
    "\n",
    "print(\"\\nüë§ Current user view:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT employee_id, first_name, last_name, ssn, department \n",
    "FROM {catalog_name}.{schema_name}.employee_records \n",
    "LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4840ba80-f1f4-413d-b22b-ac728650807e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Build Secure Tools for the Agent**\n",
    "Now we'll create Unity Catalog functions that our AI agent will use to query HR data. These functions act as the interface between the agent and our governed data.\n",
    "\n",
    "**Why UC Functions for Agent Tools?**\n",
    "- **Governed Access**: Functions inherit caller permissions \n",
    "- **Query Templates**: They validate inputs and enforce safe query patterns\n",
    "- **Audit Trail**: All function calls are logged for compliance\n",
    "- **Performance**: Functions can be optimized with proper indexing and caching\n",
    "\n",
    "**Our Agent Tool Strategy:**\n",
    "We'll create two general-purpose functions that:\n",
    "1. Work exclusively with the anonymized `data_analyst_view`\n",
    "2. Return aggregated results that can answer various HR questions\n",
    "3. Prevent the agent from writing arbitrary SQL\n",
    "4. Maintain employee privacy through anonymous IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbc6248-c526-4132-9b01-3c6390a631c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TOOL 1: Performance & Retention Analytics\n",
    "print(\"üîß Creating performance analytics function...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION\n",
    "    {catalog_name}.{schema_name}.analyze_performance()\n",
    "    RETURNS TABLE (\n",
    "        department STRING,\n",
    "        avg_rating DOUBLE,\n",
    "        min_rating DOUBLE,\n",
    "        max_rating DOUBLE,\n",
    "        employee_count INT,\n",
    "        avg_tenure_years DOUBLE\n",
    "    )\n",
    "    COMMENT 'HR Analytics: Basic performance metrics by department'\n",
    "    RETURN (\n",
    "        SELECT \n",
    "            department,\n",
    "            AVG(rating) as avg_rating,\n",
    "            MIN(rating) as min_rating,\n",
    "            MAX(rating) as max_rating,\n",
    "            COUNT(DISTINCT anonymous_id) as employee_count,\n",
    "            AVG(YEAR(CURRENT_DATE()) - hire_year) as avg_tenure_years\n",
    "        FROM {catalog_name}.{schema_name}.data_analyst_view\n",
    "        WHERE rating IS NOT NULL\n",
    "        GROUP BY department\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Created analyze_performance function\")\n",
    "print(\"  - Returns: avg/min/max ratings, employee count, avg tenure by department\")\n",
    "print(\"  - Works with: anonymized data only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20200732-9807-44b0-ad0f-63861f897f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TOOL 2: Department & Compensation Analytics\n",
    "print(\"üîß Creating operations analytics function...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION\n",
    "    {catalog_name}.{schema_name}.analyze_operations()\n",
    "    RETURNS TABLE (\n",
    "        department STRING,\n",
    "        employee_count INT,\n",
    "        avg_salary DOUBLE,\n",
    "        avg_bonus DOUBLE,\n",
    "        avg_total_comp DOUBLE,\n",
    "        avg_stock_options INT\n",
    "    )\n",
    "    COMMENT 'HR Analytics: Department compensation and operational metrics'\n",
    "    RETURN (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(DISTINCT anonymous_id) as employee_count,\n",
    "            AVG(base_salary) as avg_salary,\n",
    "            AVG(bonus) as avg_bonus,\n",
    "            AVG(base_salary + bonus) as avg_total_comp,\n",
    "            AVG(stock_options) as avg_stock_options\n",
    "        FROM {catalog_name}.{schema_name}.data_analyst_view\n",
    "        WHERE base_salary IS NOT NULL\n",
    "        GROUP BY department\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Created analyze_operations function\")\n",
    "print(\"  - Returns: compensation metrics and headcount by department\")\n",
    "print(\"  - Maintains privacy: no individual employee data exposed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eabd41-61f6-4069-a384-1fffff38f5db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 1: Performance Analytics\n",
    "print(\"\\nüìä Test 1: Performance Analytics by Department\")\n",
    "display(spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.analyze_performance()\"))\n",
    "\n",
    "# Test 2: Operations/Compensation Analytics\n",
    "print(\"\\nüìä Test 2: Compensation Analytics by Department\")\n",
    "display(spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.analyze_operations()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d0f34d-3b6f-4d54-bab0-a94c8d0058ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Governance Foundation Complete\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. **Data Classification with Tagging**: Applied sensitivity labels (Public, Internal, Confidential, Restricted) to all HR tables\n",
    "\n",
    "2. **Built Classification-Aware View**: \n",
    "   - `data_analyst_view` - Anonymous IDs, year-only dates, full compensation \n",
    "   \n",
    "3. **Configured Group-Based Access**:\n",
    "   - Set up `Devs` group with permissions on our view and models\n",
    "   - Implemented enterprise-standard permission management\n",
    "   - Ready for both users and service principals\n",
    "\n",
    "4. **Implemented Multi-Layer Security**: \n",
    "   - Table-level SSN masking (cannot be bypassed)\n",
    "   - Row-level filtering (Legal department excluded in our view)\n",
    "   - Column-level anonymization (employee IDs ‚Üí EMP_000001 format)\n",
    "\n",
    "5. **Built Secure Agent Tools**:\n",
    "   - `analyze_performance()` - Returns performance ratings and tenure by department\n",
    "   - `analyze_operations()` - Returns compensation metrics and headcount by department\n",
    "\n",
    "### The governance foundation is ready. Let's go to the next lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33c121f-dec2-475b-b470-90c7b2224887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab1_Governance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
